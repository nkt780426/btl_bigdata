{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql.functions import col, regexp_replace, trim\n",
    "import preprocess\n",
    "\n",
    "# Tạo SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimpleSparkApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy dữ liệu từ tất cả các file\n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "# Đường dẫn đến thư mục chứa các tệp JSON\n",
    "json_folder_path = \"../crawl/data/\"\n",
    "\n",
    "# Lấy danh sách các tệp trong thư mục\n",
    "json_files = [f for f in os.listdir(json_folder_path) if f.endswith(\".json\")]\n",
    "\n",
    "# Define schema for the data\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"time\", StringType(), True),\n",
    "        StructField(\"category\", ArrayType(StringType()), True),\n",
    "        StructField(\"views\", StringType(), True),\n",
    "        StructField(\"num_answer\", StringType(), True),\n",
    "        StructField(\"votes\", StringType(), True),\n",
    "        StructField(\"solved\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Tạo DataFrame rỗng để chứa dữ liệu từ tất cả các tệp JSON\n",
    "df_all = spark.createDataFrame([], schema)\n",
    "\n",
    "# Đọc từng tệp JSON và ghép vào DataFrame chung\n",
    "for json_file in json_files:\n",
    "    json_file_path = os.path.join(json_folder_path, json_file)\n",
    "    df = spark.read.json(json_file_path, schema=schema)\n",
    "\n",
    "    # Thực hiện xử lý dữ liệu như bạn đã làm trước đó\n",
    "    df_processed = (\n",
    "        df.withColumn(\"content\", regexp_replace(\"content\", \"<.*?>\", \"\"))\n",
    "        .withColumn(\"content\", regexp_replace(\"content\", \"\\\\s+\", \" \"))\n",
    "        .withColumn(\"content\", trim(col(\"content\")))\n",
    "        .withColumn(\"views\", preprocess.convert_to_numeric(\"views\"))\n",
    "        .withColumn(\"num_answer\", preprocess.convert_to_numeric(\"num_answer\"))\n",
    "        .withColumn(\"votes\", preprocess.convert_to_numeric(\"votes\"))\n",
    "    )\n",
    "\n",
    "    # Ghép DataFrame mới vào DataFrame chung\n",
    "    df_all = df_all.union(df_processed)\n",
    "\n",
    "# Loại các bản ghi chứa null hoặc có độ dài của mảng \"category\" là 0\n",
    "df_all = df_all.na.drop().filter(size(\"category\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy dữ liệu từ 1 file\n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "# Đường dẫn đến tệp JSON\n",
    "json_file_path = \"../crawl/data/tezos.json\"\n",
    "\n",
    "# Đọc tệp JSON vào DataFrame\n",
    "df = spark.read.json(json_file_path)\n",
    "\n",
    "# Loại bỏ các tag HTML và xóa khoảng trắng\n",
    "df_all = (\n",
    "    df.withColumn(\"content\", regexp_replace(\"content\", \"<.*?>\", \"\"))\n",
    "    .withColumn(\"content\", regexp_replace(\"content\", \"\\\\s+\", \" \"))\n",
    "    .withColumn(\"content\", trim(col(\"content\")))\n",
    "    .withColumn(\"views\", preprocess.convert_to_numeric(\"views\"))\n",
    "    .withColumn(\"num_answer\", preprocess.convert_to_numeric(\"num_answer\"))\n",
    "    .withColumn(\"votes\", preprocess.convert_to_numeric(\"votes\"))\n",
    ")\n",
    "\n",
    "# Loại các bản ghi chứa null hoặc có độ dài của mảng \"category\" là 0\n",
    "df_all = df_all.na.drop().filter(size(\"category\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1753827"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Đếm số bản ghi\n",
    "df_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|category       |\n",
      "+---------------+\n",
      "|operation      |\n",
      "|bigmap         |\n",
      "|conseil        |\n",
      "|tezos-client   |\n",
      "|liquidity      |\n",
      "|tokens         |\n",
      "|documentation  |\n",
      "|codebase       |\n",
      "|archetype      |\n",
      "|completium     |\n",
      "|utxo           |\n",
      "|metadata       |\n",
      "|password       |\n",
      "|burn           |\n",
      "|endorser       |\n",
      "|governance     |\n",
      "|account        |\n",
      "|staking-balance|\n",
      "|smartpy        |\n",
      "|tezbox         |\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Đếm số category\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "# Sử dụng hàm explode để chuyển mảng thành các phần tử đơn lẻ\n",
    "categories = df_all.select(explode(\"category\").alias(\"category\")).select(\"category\").distinct()\n",
    "\n",
    "categories.show(truncate=False)\n",
    "categories.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu số category\n",
    "\n",
    "# Đường dẫn đến tệp JSON đầu ra\n",
    "output_json_path = \"./output\"\n",
    "\n",
    "# Ghi DataFrame ra tệp JSON với mode là \"overwrite\"\n",
    "categories.coalesce(1).write.mode(\"overwrite\").json(output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu toàn bộ data đã qua tiền xử lý\n",
    "\n",
    "# Đường dẫn đến tệp JSON đầu ra\n",
    "output_json_path = \"./output\"\n",
    "\n",
    "# Ghi DataFrame ra tệp JSON với mode là \"overwrite\"\n",
    "df_all.coalesce(1).write.mode(\"overwrite\").json(output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, StringIndexer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Chuyển đổi cột category từ ArrayType(StringType) thành StringType\n",
    "first_category_udf = udf(lambda x: x[0] if x else None, StringType())\n",
    "df_train = df_all.limit(20000).withColumn(\"category\", first_category_udf(\"category\"))\n",
    "\n",
    "# Chuyển đổi category thành dạng số\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Tạo Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, hashingTF, rf])\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "(train_data, test_data) = df_train.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2010443864229765\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán trên tập kiểm tra\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu mô hình\n",
    "model.save(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải lại mô hình\n",
    "loaded_model = PipelineModel.load(\"your_model_path\")\n",
    "\n",
    "# Dùng mô hình đã tải lại để dự đoán\n",
    "predictions = loaded_model.transform(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đóng SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btl_bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
